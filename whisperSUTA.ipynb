{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allens/anaconda3/envs/tta/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import whisper\n",
    "from whisper.audio import (\n",
    "    log_mel_spectrogram,\n",
    "    pad_or_trim,\n",
    "    load_audio,\n",
    ")\n",
    "\n",
    "import jiwer\n",
    "from tqdm import tqdm\n",
    "from main import *\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"base.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['encoder.conv1.weight', 'encoder.conv1.bias', 'encoder.conv2.weight', 'encoder.conv2.bias', 'encoder.blocks.0.attn_ln.weight', 'encoder.blocks.0.attn_ln.bias', 'encoder.blocks.0.mlp_ln.weight', 'encoder.blocks.0.mlp_ln.bias', 'encoder.blocks.1.attn_ln.weight', 'encoder.blocks.1.attn_ln.bias', 'encoder.blocks.1.mlp_ln.weight', 'encoder.blocks.1.mlp_ln.bias', 'encoder.blocks.2.attn_ln.weight', 'encoder.blocks.2.attn_ln.bias', 'encoder.blocks.2.mlp_ln.weight', 'encoder.blocks.2.mlp_ln.bias', 'encoder.blocks.3.attn_ln.weight', 'encoder.blocks.3.attn_ln.bias', 'encoder.blocks.3.mlp_ln.weight', 'encoder.blocks.3.mlp_ln.bias', 'encoder.blocks.4.attn_ln.weight', 'encoder.blocks.4.attn_ln.bias', 'encoder.blocks.4.mlp_ln.weight', 'encoder.blocks.4.mlp_ln.bias', 'encoder.blocks.5.attn_ln.weight', 'encoder.blocks.5.attn_ln.bias', 'encoder.blocks.5.mlp_ln.weight', 'encoder.blocks.5.mlp_ln.bias', 'encoder.ln_post.weight', 'encoder.ln_post.bias', 'decoder.blocks.0.attn_ln.weight', 'decoder.blocks.0.attn_ln.bias', 'decoder.blocks.0.cross_attn_ln.weight', 'decoder.blocks.0.cross_attn_ln.bias', 'decoder.blocks.0.mlp_ln.weight', 'decoder.blocks.0.mlp_ln.bias', 'decoder.blocks.1.attn_ln.weight', 'decoder.blocks.1.attn_ln.bias', 'decoder.blocks.1.cross_attn_ln.weight', 'decoder.blocks.1.cross_attn_ln.bias', 'decoder.blocks.1.mlp_ln.weight', 'decoder.blocks.1.mlp_ln.bias', 'decoder.blocks.2.attn_ln.weight', 'decoder.blocks.2.attn_ln.bias', 'decoder.blocks.2.cross_attn_ln.weight', 'decoder.blocks.2.cross_attn_ln.bias', 'decoder.blocks.2.mlp_ln.weight', 'decoder.blocks.2.mlp_ln.bias', 'decoder.blocks.3.attn_ln.weight', 'decoder.blocks.3.attn_ln.bias', 'decoder.blocks.3.cross_attn_ln.weight', 'decoder.blocks.3.cross_attn_ln.bias', 'decoder.blocks.3.mlp_ln.weight', 'decoder.blocks.3.mlp_ln.bias', 'decoder.blocks.4.attn_ln.weight', 'decoder.blocks.4.attn_ln.bias', 'decoder.blocks.4.cross_attn_ln.weight', 'decoder.blocks.4.cross_attn_ln.bias', 'decoder.blocks.4.mlp_ln.weight', 'decoder.blocks.4.mlp_ln.bias', 'decoder.blocks.5.attn_ln.weight', 'decoder.blocks.5.attn_ln.bias', 'decoder.blocks.5.cross_attn_ln.weight', 'decoder.blocks.5.cross_attn_ln.bias', 'decoder.blocks.5.mlp_ln.weight', 'decoder.blocks.5.mlp_ln.bias', 'decoder.ln.weight', 'decoder.ln.bias']\n"
     ]
    }
   ],
   "source": [
    "# collect trainable params\n",
    "params = []\n",
    "names = []\n",
    "for nm, m in model.named_modules():\n",
    "    # print(str(nm).split('.'))\n",
    "    trainable = ['weight', 'bias']\n",
    "    # train_LN\n",
    "    if isinstance(m, nn.LayerNorm):\n",
    "        for np, p in m.named_parameters():\n",
    "            if np in trainable:  \n",
    "                p.requires_grad = True\n",
    "                params.append(p)\n",
    "                names.append(f\"{nm}.{np}\")\n",
    "    # train_feature\n",
    "    if len(str(nm).split('.')) > 1:\n",
    "        if str(nm).split('.')[0] == 'encoder' and (str(nm).split('.')[1] == 'conv1' or str(nm).split('.')[1] == 'conv2'):\n",
    "            for np, p in m.named_parameters():\n",
    "                p.requires_grad = True\n",
    "                params.append(p)\n",
    "                names.append(f\"{nm}.{np}\")\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load audio\n",
    "model = model.to(DEVICE)\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "audio = load_audio(file='./p232_001.wav')\n",
    "audio = pad_or_trim(audio)\n",
    "mel = log_mel_spectrogram(audio)\n",
    "mel = mel.unsqueeze(-1)\n",
    "mel = mel.permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward\n",
    "mel = mel.to(DEVICE)\n",
    "outputs = model.decode(mel, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tensor = torch.stack(outputs[1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 51864])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tensor=result_tensor.permute(1,0,2)\n",
    "result_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6453, device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_loss = softmax_entropy(result_tensor).mean(0).mean()\n",
    "e_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 51864])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 10.02 GiB. GPU 0 has a total capacty of 11.72 GiB of which 1020.44 MiB is free. Including non-PyTorch memory, this process has 10.52 GiB memory in use. Of the allocated memory 10.31 GiB is allocated by PyTorch, and 9.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmcc_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Test-time-adaptation-ASR-SUTA/main.py:39\u001b[0m, in \u001b[0;36mmcc_loss\u001b[0;34m(x, reweight, dim, class_num)\u001b[0m\n\u001b[1;32m     37\u001b[0m     cov_matrix_t \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mmul(target_entropy_weight\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmm(p)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:    \n\u001b[0;32m---> 39\u001b[0m     cov_matrix_t \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (D, L) * (L, D) -> (D, D)\u001b[39;00m\n\u001b[1;32m     41\u001b[0m cov_matrix_t \u001b[38;5;241m=\u001b[39m cov_matrix_t \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(cov_matrix_t, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m mcc_loss \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39msum(cov_matrix_t) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtrace(cov_matrix_t)) \u001b[38;5;241m/\u001b[39m class_num\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 10.02 GiB. GPU 0 has a total capacty of 11.72 GiB of which 1020.44 MiB is free. Including non-PyTorch memory, this process has 10.52 GiB memory in use. Of the allocated memory 10.31 GiB is allocated by PyTorch, and 9.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "c_loss = mcc_loss(result_tensor, reweight=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "loss += e_loss * 0.5\n",
    "loss+= c_loss * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt\n",
    "loss = 0\n",
    "not_blank = True\n",
    "if em_coef > 0: \n",
    "    if not_blank:      \n",
    "        e_loss = softmax_entropy(outputs / temp)[non_blank].mean(0).mean()\n",
    "\n",
    "    else: \n",
    "        e_loss = softmax_entropy(outputs / temp).mean(0).mean() \n",
    "    \n",
    "    loss += e_loss * em_coef\n",
    "\n",
    "if 1 - em_coef > 0: \n",
    "    c_loss = mcc_loss(outputs / temp, reweight)\n",
    "    loss += c_loss * (1 - em_coef)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "if scheduler is not None: \n",
    "    scheduler.step()\n",
    "model.zero_grad()\n",
    "\n",
    "# inference again\n",
    "if repeat_inference:\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x).logits\n",
    "return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(DEVICE)\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1500, 512])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder(mel).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.decode(mel.to(DEVICE), options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DecodingResult(audio_features=tensor([[-4.9072e-01, -2.8223e-01, -1.9910e-01,  ...,  9.4727e-01,\n",
       "          -5.4541e-01,  5.7324e-01],\n",
       "         [ 6.1426e-01,  3.9844e-01,  5.8380e-02,  ...,  2.8638e-01,\n",
       "          -1.3696e-01,  6.5967e-01],\n",
       "         [ 6.5234e-01,  6.8164e-01,  7.3181e-02,  ...,  1.2720e-01,\n",
       "          -4.4824e-01,  1.2455e-03],\n",
       "         ...,\n",
       "         [-5.0995e-02, -3.2135e-02, -3.2227e-02,  ..., -2.5711e-03,\n",
       "           8.3847e-03, -1.8280e-02],\n",
       "         [-1.7012e+00, -2.0837e-01,  4.0015e-01,  ...,  3.6743e-01,\n",
       "          -1.3389e+00,  8.7012e-01],\n",
       "         [-7.9834e-01, -2.4451e-01,  3.3179e-01,  ...,  4.9292e-01,\n",
       "          -1.0127e+00,  2.0264e-01]], device='cuda:0', dtype=torch.float16), language='en', language_probs=None, tokens=[4222, 869, 45856, 13], text='Please call Stella.', avg_logprob=-0.3159170627593994, no_speech_prob=0.062296539545059204, temperature=0.0, compression_ratio=0.7037037037037037)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please call Stella.\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
