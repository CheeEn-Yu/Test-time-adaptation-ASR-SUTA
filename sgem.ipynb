{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read text: 100%|██████████| 2939/2939 [00:00<00:00, 75374.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]    There are 2939 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import whisper\n",
    "from dataclasses import dataclass, field, replace\n",
    "from whisper.decoding import DecodingTask\n",
    "from whisper.audio import (\n",
    "    log_mel_spectrogram,\n",
    "    pad_or_trim,\n",
    "    load_audio,\n",
    ")\n",
    "\n",
    "seed = 42\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "from jiwer import wer\n",
    "\n",
    "from data import *\n",
    "from suta import *\n",
    "from omegaconf import OmegaConf\n",
    "args = OmegaConf.load(\"config.yaml\")\n",
    "\n",
    "dataset = load_dataset(['test-other'], 'librispeech', 'LibriSpeech', 1, extra_noise=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_tokens = []\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "normalizer = EnglishTextNormalizer()\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "exp_name = 'ex_data/suta_ex'\n",
    "with open(f'{exp_name}/transcript.txt', 'a') as f:\n",
    "\n",
    "    for count, batch in tqdm(enumerate(dataset)):\n",
    "        if count > 100:\n",
    "            break\n",
    "        # load model\n",
    "        model = whisper.load_model(args.asr)\n",
    "        model.eval()\n",
    "        task = DecodingTask(model, options)\n",
    "\n",
    "        # set training param\n",
    "        params, names = whisper_collect_params(model, args.encoderLN, args.decoderLN, train_feature=args.train_feature)\n",
    "        optimizer, scheduler = setup_optimizer(params, args.opt, 2e-5, scheduler=None)\n",
    "\n",
    "        # unzip batch\n",
    "        lens, wavs, texts, files = batch\n",
    "        f.write(f'idx:{count}'+'\\n')\n",
    "        f.write('label:'+normalizer(texts[0])+'\\n')\n",
    "\n",
    "        # preprocess data\n",
    "        if args.asr == 'large' or args.asr == 'large_v2' or args.asr == 'large_v3': # the code is for batch size = 1\n",
    "            mel = log_mel_spectrogram(pad_or_trim(wavs[0]), n_mels=128).unsqueeze(0).to(DEVICE)\n",
    "        else:\n",
    "            mel = log_mel_spectrogram(pad_or_trim(wavs[0])).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        losses = []\n",
    "        wers = []\n",
    "        # original whisper output\n",
    "        with torch.no_grad():\n",
    "            result = model.decode(mel, options)[0]\n",
    "            teacher_tokens = result.tokens\n",
    "            text = result.text\n",
    "            ori_wer = wer(normalizer(texts[0]), normalizer(text))\n",
    "            wers.append(ori_wer)\n",
    "        f.write(f'ori({ori_wer}):{text}\\n')\n",
    "        del result\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # teacher forcing to get logit\n",
    "        options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "        task = DecodingTask(model, options)\n",
    "        if options.beam_size is not None:\n",
    "            n_batch = options.beam_size\n",
    "        else:\n",
    "            n_batch = 1\n",
    "\n",
    "        # SGEM or SUTA\n",
    "        for step in range(args.steps):\n",
    "            model.zero_grad()\n",
    "            audio_features = task._get_audio_features(mel)\n",
    "            tokens = torch.tensor([task.initial_tokens]).repeat(1, 1).to(device=audio_features.device)\n",
    "            tokens = tokens.repeat_interleave(task.n_group, dim=0).to(audio_features.device)\n",
    "            sum_logprobs = torch.zeros(n_batch, device=audio_features.device)\n",
    "\n",
    "            loss = 0\n",
    "            entropy_list = None\n",
    "            negative_loss = 0\n",
    "            for i in range(len(teacher_tokens)):\n",
    "                added_token = torch.Tensor([[teacher_tokens[i]]]).long().expand(tokens.shape[0], 1).to(DEVICE)\n",
    "                tokens = torch.cat((tokens, added_token), dim=1)\n",
    "                logits = task.inference.logits(tokens, audio_features) # (1,2,51864)\n",
    "                \n",
    "                logits = logits[:, -1]\n",
    "                for logit_filter in task.logit_filters:\n",
    "                    logit_filter.apply(logits, tokens)\n",
    "                \n",
    "                logits = torch.topk(logits, k=30).values\n",
    "\n",
    "                # SUTA\n",
    "                e_loss = softmax_entropy(logits.unsqueeze(0) / args.temp).mean(0).mean()\n",
    "                loss += e_loss * args.em_coef\n",
    "                c_loss = mcc_loss(logits.unsqueeze(0) / args.temp, class_num=args.topk)\n",
    "                loss += c_loss * (1 - args.em_coef)\n",
    "\n",
    "                # # GEM\n",
    "                # entropy = torch.log(torch.pow(logits.softmax(dim=-1), args.renyi_entropy_alpha).sum(dim=-1)) # entropy: B, L\n",
    "                # entropy = entropy / (1 - args.renyi_entropy_alpha)\n",
    "                # entropy = entropy.mean()\n",
    "                # if entropy_list is None:\n",
    "                #     entropy_list = entropy.unsqueeze(0)\n",
    "                # else:\n",
    "                #     entropy_list = torch.cat((entropy.unsqueeze(0), entropy_list), dim=-1)\n",
    "\n",
    "                # # NS\n",
    "                # negative_outputs = logits.clone()\n",
    "                # negative_mask = torch.where(torch.softmax(negative_outputs, dim=-1) < args.ns_threshold * (10 / negative_outputs.shape[-1]), 1, 0)\n",
    "                # negative_loss += torch.mean(-torch.log(1 - torch.sum(negative_mask * torch.softmax(negative_outputs / args.temp, dim=-1), dim=-1)))\n",
    "\n",
    "            # e_loss = entropy_list.mean()\n",
    "            # loss = args.ns_coef * negative_loss + e_loss\n",
    "            losses.append(loss)\n",
    "            if step==0:\n",
    "                losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            task.inference.cleanup_caching()\n",
    "\n",
    "            # output after adaptation\n",
    "            options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "            task = DecodingTask(model, options)\n",
    "            with torch.no_grad():\n",
    "                after_text = model.decode(mel, options)[0].text\n",
    "            after_wer = wer(normalizer(texts[0]), normalizer(after_text))\n",
    "            f.write(f'step{step}({after_wer}): {after_text}\\n')\n",
    "            wers.append(after_wer)\n",
    "            del logits\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # plot loss curve and wer\n",
    "        fig, ax1 = plt.subplots()\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('step')\n",
    "        ax1.set_ylabel('loss', color=color)\n",
    "        ax1.plot([loss.cpu().detach() for loss in losses], color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        # 在右側 y 軸上繪製 data2\n",
    "        ax2 = ax1.twinx()  # 共享 x 軸\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('wer', color=color)\n",
    "        ax2.plot(wers, color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        plt.title(f'idx:{count}')\n",
    "        plt.savefig(f'./ex_data/suta_ex/suta_{count}.png')\n",
    "\n",
    "        f.write(\"=======================================\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_tokens = []\n",
    "from whisper.normalizers import EnglishTextNormalizer\n",
    "normalizer = EnglishTextNormalizer()\n",
    "options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "exp_name = 'ex_data/sgem_ex'\n",
    "with open(f'{exp_name}/transcript.txt', 'a') as f:\n",
    "\n",
    "    for count, batch in tqdm(enumerate(dataset)):\n",
    "        if count > 100:\n",
    "            break\n",
    "        # load model\n",
    "        model = whisper.load_model(args.asr)\n",
    "        model.eval()\n",
    "        task = DecodingTask(model, options)\n",
    "\n",
    "        # set training param\n",
    "        params, names = whisper_collect_params(model, args.encoderLN, args.decoderLN, train_feature=args.train_feature)\n",
    "        optimizer, scheduler = setup_optimizer(params, args.opt, 2e-5, scheduler=None)\n",
    "\n",
    "        # unzip batch\n",
    "        lens, wavs, texts, files = batch\n",
    "        f.write(f'idx:{count}'+'\\n')\n",
    "        f.write('label:'+normalizer(texts[0])+'\\n')\n",
    "\n",
    "        # preprocess data\n",
    "        if args.asr == 'large' or args.asr == 'large_v2' or args.asr == 'large_v3': # the code is for batch size = 1\n",
    "            mel = log_mel_spectrogram(pad_or_trim(wavs[0]), n_mels=128).unsqueeze(0).to(DEVICE)\n",
    "        else:\n",
    "            mel = log_mel_spectrogram(pad_or_trim(wavs[0])).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "        losses = []\n",
    "        wers = []\n",
    "        # original whisper output\n",
    "        with torch.no_grad():\n",
    "            result = model.decode(mel, options)[0]\n",
    "            teacher_tokens = result.tokens\n",
    "            text = result.text\n",
    "            ori_wer = wer(normalizer(texts[0]), normalizer(text))\n",
    "            wers.append(ori_wer)\n",
    "        f.write(f'ori({ori_wer}):{text}\\n')\n",
    "        del result\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # teacher forcing to get logit\n",
    "        options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "        task = DecodingTask(model, options)\n",
    "        if options.beam_size is not None:\n",
    "            n_batch = options.beam_size\n",
    "        else:\n",
    "            n_batch = 1\n",
    "\n",
    "        # SGEM or SUTA\n",
    "        for step in range(args.steps):\n",
    "            model.zero_grad()\n",
    "            audio_features = task._get_audio_features(mel)\n",
    "            tokens = torch.tensor([task.initial_tokens]).repeat(1, 1).to(device=audio_features.device)\n",
    "            tokens = tokens.repeat_interleave(task.n_group, dim=0).to(audio_features.device)\n",
    "            sum_logprobs = torch.zeros(n_batch, device=audio_features.device)\n",
    "\n",
    "            loss = 0\n",
    "            entropy_list = None\n",
    "            negative_loss = 0\n",
    "            for i in range(len(teacher_tokens)):\n",
    "                added_token = torch.Tensor([[teacher_tokens[i]]]).long().expand(tokens.shape[0], 1).to(DEVICE)\n",
    "                tokens = torch.cat((tokens, added_token), dim=1)\n",
    "                logits = task.inference.logits(tokens, audio_features) # (1,2,51864)\n",
    "                \n",
    "                logits = logits[:, -1]\n",
    "                for logit_filter in task.logit_filters:\n",
    "                    logit_filter.apply(logits, tokens)\n",
    "                \n",
    "                logits = torch.topk(logits, k=30).values\n",
    "\n",
    "                # # SUTA\n",
    "                # e_loss = softmax_entropy(logits.unsqueeze(0) / args.temp).mean(0).mean()\n",
    "                # loss += e_loss * args.em_coef\n",
    "                # c_loss = mcc_loss(logits.unsqueeze(0) / args.temp, class_num=args.topk)\n",
    "                # loss += c_loss * (1 - args.em_coef)\n",
    "\n",
    "                # GEM\n",
    "                entropy = torch.log(torch.pow(logits.softmax(dim=-1), args.renyi_entropy_alpha).sum(dim=-1)) # entropy: B, L\n",
    "                entropy = entropy / (1 - args.renyi_entropy_alpha)\n",
    "                entropy = entropy.mean()\n",
    "                if entropy_list is None:\n",
    "                    entropy_list = entropy.unsqueeze(0)\n",
    "                else:\n",
    "                    entropy_list = torch.cat((entropy.unsqueeze(0), entropy_list), dim=-1)\n",
    "\n",
    "                # NS\n",
    "                negative_outputs = logits.clone()\n",
    "                negative_mask = torch.where(torch.softmax(negative_outputs, dim=-1) < args.ns_threshold * (10 / negative_outputs.shape[-1]), 1, 0)\n",
    "                negative_loss += torch.mean(-torch.log(1 - torch.sum(negative_mask * torch.softmax(negative_outputs / args.temp, dim=-1), dim=-1)))\n",
    "\n",
    "            e_loss = entropy_list.mean()\n",
    "            loss = args.ns_coef * negative_loss + e_loss\n",
    "            losses.append(loss)\n",
    "            if step==0:\n",
    "                losses.append(loss)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            task.inference.cleanup_caching()\n",
    "\n",
    "            # output after adaptation\n",
    "            options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "            task = DecodingTask(model, options)\n",
    "            with torch.no_grad():\n",
    "                after_text = model.decode(mel, options)[0].text\n",
    "            after_wer = wer(normalizer(texts[0]), normalizer(after_text))\n",
    "            f.write(f'step{step}({after_wer}): {after_text}\\n')\n",
    "            wers.append(after_wer)\n",
    "            del logits, negative_outputs\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # plot loss curve and wer\n",
    "        fig, ax1 = plt.subplots()\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('step')\n",
    "        ax1.set_ylabel('loss', color=color)\n",
    "        ax1.plot([loss.cpu().detach() for loss in losses], color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        # 在右側 y 軸上繪製 data2\n",
    "        ax2 = ax1.twinx()  # 共享 x 軸\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('wer', color=color)\n",
    "        ax2.plot(wers, color=color)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        plt.title(f'idx:{count}')\n",
    "        plt.savefig(f'./ex_data/sgem_ex/sgem_{count}.png')\n",
    "\n",
    "        f.write(\"=======================================\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不同的單字: {'his', 'casting,', 'was', 'this', 'casting', 'is'}\n"
     ]
    }
   ],
   "source": [
    "def find_different_words(sentence1, sentence2):\n",
    "    # 將句子轉換成集合，其中集合會自動去除重複的單字\n",
    "    words_set1 = set(sentence1.split())\n",
    "    words_set2 = set(sentence2.split())\n",
    "    \n",
    "    # 找出兩個句子中不同的單字\n",
    "    different_words = words_set1.symmetric_difference(words_set2)\n",
    "    \n",
    "    return different_words\n",
    "\n",
    "# 例子\n",
    "sentence1 = \"They've indigently over the board. Each watching with feverish anxiety was companions movements. Each casting, now and again, a gloating eye upon the keep of gold and greenbacks that lay between them. And at times, half stretching out this hand, watching.\"\n",
    "sentence2 = \"They've indigently over the board. Each watching with feverish anxiety is companions movements. Each casting now and again, a gloating eye upon the keep of gold and greenbacks that lay between them. And at times, half stretching out his hand, watching.\"\n",
    "result = find_different_words(sentence1, sentence2)\n",
    "print(\"不同的單字:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ori_wer)):\n",
    "    if ori_wer[i] - after_wer[i] < -0.2:\n",
    "        print(ori_wer[i])\n",
    "        print(after_wer[i])\n",
    "        print('===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23204044255065828"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(ori_wer).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23295592382468738"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(after_wer).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ori whisper code\n",
    "count = 0\n",
    "options = whisper.DecodingOptions(language=\"en\", beam_size=4, without_timestamps=True)\n",
    "for batch in tqdm(dataset):\n",
    "    count+=1\n",
    "    if count > 1:\n",
    "        break\n",
    "    model = whisper.load_model(args.asr)\n",
    "    model.eval()\n",
    "    task = DecodingTask(model, options)\n",
    "    task.decoder.reset()\n",
    "    lens, wavs, texts, files = batch\n",
    "    # the code is for batch size = 1\n",
    "    if args.asr == 'large' or args.asr == 'large_v2' or args.asr == 'large_v3':\n",
    "        mel = log_mel_spectrogram(pad_or_trim(wavs[0]), n_mels=128).unsqueeze(0).to(DEVICE)\n",
    "    else:\n",
    "        mel = log_mel_spectrogram(pad_or_trim(wavs[0])).unsqueeze(0).to(DEVICE)\n",
    "    audio_features = task._get_audio_features(mel)\n",
    "    tokens = torch.tensor([task.initial_tokens]).repeat(1, 1).to(device=audio_features.device)\n",
    "    tokens = tokens.repeat_interleave(task.n_group, dim=0).to(audio_features.device)\n",
    "    languages, language_probs = task._detect_language(audio_features, tokens)\n",
    "    n_batch = tokens.shape[0]\n",
    "    sum_logprobs = torch.zeros(n_batch, device=audio_features.device)\n",
    "    try:\n",
    "        for i in range(task.sample_len):\n",
    "            logits = task.inference.logits(tokens, audio_features) # (1,2,51864)\n",
    "            if (\n",
    "                i == 0 and task.tokenizer.no_speech is not None\n",
    "            ):  # save no_speech_probs\n",
    "                probs_at_sot = logits[:, task.sot_index].float().softmax(dim=-1)\n",
    "                no_speech_probs = probs_at_sot[:, task.tokenizer.no_speech].tolist()\n",
    "\n",
    "            # now we need to consider the logits at the last tokens only\n",
    "            logits = logits[:, -1]\n",
    "            print(logits)\n",
    "            raise\n",
    "\n",
    "            # apply the logit filters, e.g. for suppressing or applying penalty to\n",
    "            for logit_filter in task.logit_filters:\n",
    "                logit_filter.apply(logits, tokens)\n",
    "\n",
    "            # expand the tokens tensor with the selected next tokens\n",
    "            tokens, completed, logit_rank = task.decoder.update(tokens, logits, sum_logprobs)\n",
    "\n",
    "            if completed or tokens.shape[-1] > task.n_ctx:\n",
    "                break\n",
    "    finally:\n",
    "        task.inference.cleanup_caching()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1500, 384])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# audio feature shape\n",
    "after_text[0].audio_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1302, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_loss = entropy_list.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0887, device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.ns_coef * negative_loss + e_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 51865])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51777"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_inf_idx = torch.isfinite(logits)\n",
    "num_finite_values = torch.sum(non_inf_idx).item()\n",
    "num_finite_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51777])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_inf_idx = torch.isfinite(logit_arr[0])\n",
    "logit_arr[0][non_inf_idx].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[22.3750, 21.5938, 20.7500, 19.7500, 19.5938, 19.5156, 19.0938, 18.9375,\n",
       "         18.9219, 18.8906, 18.7812, 18.6562, 18.6250, 18.5625, 18.4219, 18.4062,\n",
       "         18.2969, 18.2656, 18.2344, 18.2031, 18.1406, 18.0625, 18.0625, 18.0312,\n",
       "         18.0156, 17.9531, 17.9375, 17.8750, 17.8438, 17.7969]],\n",
       "       device='cuda:0', grad_fn=<TopkBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.topk(logits,k=30).values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
